---
---

@inproceedings{litschko-etal-2025-cdir,
    title = "Cross-Dialect Information Retrieval: Information Access in Low-Resource and High-Variance Languages",
    author = "Litschko, Robert  and
      Kraus, Oliver  and
      Blaschke, Verena  and
      Plank, Barbara",
    editor = "",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics (COLING)",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    pdf = "https://aclanthology.org/2025.coling-main.678/",
    pages = "10158--10171",
    abstract = "A large amount of local and culture-specific knowledge (e.g., people, traditions, food) can only be found in documents written in dialects. While there has been extensive research conducted on cross-lingual information retrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received limited attention. Dialect retrieval poses unique challenges due to the limited availability of resources to train retrieval models and the high variability in non-standardized languages. We study these challenges on the example of German dialects and introduce the first German dialect retrieval dataset, dubbed WikiDIR, which consists of seven German dialects extracted from Wikipedia. Using WikiDIR, we demonstrate the weakness of lexical methods in dealing with high lexical variation in dialects. We further show that commonly used zero-shot cross-lingual transfer approach with multilingual encoders do not transfer well to extremely low-resource setups, motivating the need for resource-lean and dialect-specific retrieval models. We finally demonstrate that (document) translation is an effective way to reduce the dialect gap in CDIR.",
    preview = {cdir.png},
    selected={true},
    code = {https://github.com/mainlp/WikiDIR},
    slides = {cdir_coling-2025-slides.pdf}
}

@article{litschko2024resource,
  title={Resource-lean transfer methods for cross-lingual information retrieval},
  author={Litschko, Robert},
  journal={Dissertation},
  pdf={https://madoc.bib.uni-mannheim.de/68318/},
  abstract="Cross-Lingual Information Retrieval (CLIR) is the task of finding relevant documents written in a language different from the query language. Neural machine translation systems and CLIR models based on supervised machine learning (deep learning) are resource-hungry approaches requiring large amounts of training data, which is expensive to obtain and therefore does not scale well to a large number of languages. In this thesis, we study methods for transferring retrieval models across languages in a resource-lean way. The overarching goal is to build effective CLIR systems for languages for which we do not have access to large-scale training data. On a high level, our contributions fall into three areas. Unsupervised learning of CLIR models. In the first part, we propose two fully unsupervised neural CLIR approaches for which no relevance annotations are required. In the representation-based approach, we encode queries and documents into independent semantic vector representations and use vector space similarity measures to calculate document relevance scores. Here, we obtain aligned query and document representations from static cross-lingual word embeddings (CLWEs) and contextual representations produced by multilingual text encoders. In the term-by-term query translation approach, we translate query terms by replacing their occurrences with their cross-lingual nearest neighbors found in CLWE spaces, effectively casting CLIR into a noisy variant of monolingual IR (MoIR). We conduct a large-scale evaluation and, surprisingly, find that off-the-shelf multilingual text encoders fall behind CLWE-based methods in a direct comparison, whereas further specialization for sentence-level semantics yields the best results. Resource-lean transfer of CLIR models. In the second part, we focus on the standard zero-shot cross-lingual transfer (ZS-XLT) setup and use English training data to transfer cross-encoder (CE) reranking models to other languages. We first show that this approach suffers from ``monolingual overfitting'' where models are biased towards lexical matches between query and document tokens. To regularize this bias, we propose to train CEs on code-switched data instead. Our results show that this consistently improves the ZS-XLT performance for CLIR and maintains stable performance in MoIR. Next, we rely on parameter-efficient transfer methods to disentangle the task of learning-to-rank from learning target language semantics. We show that this modular approach improves upon the standard ZS-XLT approach in a scenario where the training and test data are in different domains. In the third part, we present on the example task of multilingual dependency parsing a proof of concept for instance-level model selection. Here, we propose cross-lingual transfer with multiple monolingual expert models by using a routing model. Moving away from a single multilingual model bypasses any capacity limits in terms of number of languages (``curse of multilinguality''). Our results pave the way for future work on CLIR involving multiple encoders (e.g. language-family specific encoders).",
  year={2024},
  month = dec,
  preview={diss.png},
  slides={disputation.pdf}
}

@inproceedings{sedova-etal-2024-know,
    title = "To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity",
    author = "Sedova*, Anastasiia  and
      Litschko*, Robert  and
      Frassinelli, Diego  and
      Roth, Benjamin  and
      Plank, Barbara",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2024.findings-emnlp.1003/",
    pages = "17203--17217",
    abstract = "One of the major aspects contributing to the striking performance of large language models (LLMs) is the vast amount of factual knowledge accumulated during pre-training. Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability. This paper focuses on entity type ambiguity, analyzing the proficiency and consistency of state-of-the-art LLMs in applying factual knowledge when prompted with ambiguous entities. To do so, we propose an evaluation protocol that disentangles knowing from applying knowledge, and test state-of-the-art LLMs on 49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing the correct entity reading, achieving an average accuracy of only 85{\%}, and as low as 75{\%} with underspecified prompts. The results also reveal systematic discrepancies in LLM behavior, showing that while the models may possess knowledge, they struggle to apply it consistently, exhibit biases toward preferred readings, and display self-inconsistencies. This highlights the need to address entity ambiguity in the future for more trustworthy LLMs.",
    selected={true},
    preview={to-know-or-not-to-know.png},
    slides={Know_not_know_slides.pdf},
    video={https://drive.google.com/file/d/1Tllq1PKXTmEu2DiSXLII2SAxC7_vrze3/view},
    code={https://github.com/anasedova/ToKnow_or_NotToKnow},
    poster={emnlp23_know_not_know_poster.pdf}
}


@inproceedings{chen-etal-2024-seeing,
    title = "{\textquotedblleft}Seeing the Big through the Small{\textquotedblright}: Can {LLM}s Approximate Human Judgment Distributions on {NLI} from a Few Explanations?",
    author = "Chen, Beiduo  and
      Wang, Xinpeng  and
      Peng, Siyao  and
      Litschko, Robert  and
      Korhonen, Anna  and
      Plank, Barbara",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2024.findings-emnlp.842/",
    pages = "14396--14419",
    abstract = "Human label variation (HLV) is a valuable source of information that arises when multiple human annotators provide different labels for valid reasons. In Natural Language Inference (NLI) earlier approaches to capturing HLV involve either collecting annotations from many crowd workers to represent human judgment distribution (HJD) or use expert linguists to provide detailed explanations for their chosen labels. While the former method provides denser HJD information, obtaining it is resource-intensive. In contrast, the latter offers richer textual information but it is challenging to scale up to many human judges. Besides, large language models (LLMs) are increasingly used as evaluators ({\textquotedblleft}LLM judges{\textquotedblright}) but with mixed results, and few works aim to study HJDs. This study proposes to exploit LLMs to approximate HJDs using a small number of expert labels and explanations. Our experiments show that a few explanations significantly improve LLMs' ability to approximate HJDs with and without explicit labels, thereby providing a solution to scale up annotations for HJD. However, fine-tuning smaller soft-label aware models with the LLM-generated model judgment distributions (MJDs) presents partially inconsistent results: while similar in distance, their resulting fine-tuned models and visualized distributions differ substantially. We show the importance of complementing instance-level distance measures with a global-level shape metric and visualization to more effectively evaluate MJDs against human judgment distributions.",
    preview = {big-through-small.png},
    code = {https://github.com/mainlp/MJD-Estimator},
    video = {https://drive.google.com/file/d/1AwkF3DV-4Yvc0ZCFqb3wdDL9oMDjxWdi/view},
    slides={EMNLP24_SeeBFromS_Slides.pdf},
    poster={EMNLP2024_MJDE_poster.pdf}
}


@article{sabeh2024exploring,
  title={Exploring Large Language Models for Product Attribute Value Identification},
  author={Sabeh, Kassem and Kacimi, Mouna and Gamper, Johann and Litschko, Robert and Plank, Barbara},
  journal={arXiv preprint arXiv:2409.12695},
  abstract="Product attribute value identification (PAVI) involves automatically identifying attributes and their values from product information, enabling features like product search, recommendation, and comparison. Existing methods primarily rely on fine-tuning pre-trained language models, such as BART and T5, which require extensive task-specific training data and struggle to generalize to new attributes. This paper explores large language models (LLMs), such as LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We propose various strategies: comparing one-step and two-step prompt-based approaches in zero-shot settings and utilizing parametric and non-parametric knowledge through in-context learning examples. We also introduce a dense demonstration retriever based on a pre-trained T5 model and perform instruction fine-tuning to explicitly train LLMs on task-specific instructions. Extensive experiments on two product benchmarks show that our two-step approach significantly improves performance in zero-shot settings, and instruction fine-tuning further boosts performance when using training data, demonstrating the practical benefits of using LLMs for PAVI.",
  year={2024},
  pdf={https://arxiv.org/abs/2409.12695},
  preview={llm-pavi.png}
}


@article{sabeh2024empirical,
  title={An empirical comparison of generative approaches for product attribute-value identification},
  author={Sabeh, Kassem and Litschko, Robert and Kacimi, Mouna and Plank, Barbara and Gamper, Johann},
  journal={arXiv preprint arXiv:2407.01137},
  abstract={Product attributes are crucial for e-commerce platforms, supporting applications like search, recommendation, and question answering. The task of Product Attribute and Value Identification (PAVI) involves identifying both attributes and their values from product information. In this paper, we formulate PAVI as a generation task and provide, to the best of our knowledge, the most comprehensive evaluation of PAVI so far. We compare three different attribute-value generation (AVG) strategies based on fine-tuning encoder-decoder models on three datasets. Experiments show that end-to-end AVG approach, which is computationally efficient, outperforms other strategies. However, there are differences depending on model sizes and the underlying language model.},
  year={2024},
  pdf={https://arxiv.org/abs/2407.01137},
  preview={empirical-pavi.png}
}


@inproceedings{zhou-etal-2024-mainlp,
    title = "{M}ai{NLP} at {S}em{E}val-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness",
    author = "Zhou, Shijia  and
      Shan, Huangyan  and
      Plank, Barbara  and
      Litschko, Robert",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Tayyar Madabushi, Harish  and
      Da San Martino, Giovanni  and
      Rosenthal, Sara  and
      Ros{\'a}, Aiala},
    booktitle = "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024), co-located with NAACL",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2024.semeval-1.259/",
    pages = "1842--1853",
    abstract = "This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to detect semantic relatedness of two sentences from the same languages. For cross-lingual approach we developed a set of linguistics-inspired models trained with several task-specific strategies. We 1) utilize language vectors for selection of donor languages; 2) investigate the multi-source approach for training; 3) use transliteration of non-latin script to study impact of {\textquotedblleft}script gap{\textquotedblright}; 4) opt machine translation for data augmentation. We additionally compare the performance of XLM-RoBERTa and Furina with the same training strategy. Our submission achieved the first place in the C8 (Kinyarwanda) test.",
    preview = {mainlp.png},
    code = {https://github.com/mainlp/SemEval-2024-Task-1}
}


@inproceedings{weber-etal-2024-donkii,
    title = "Donkii: Characterizing and Detecting Errors in Instruction-Tuning Datasets",
    author = "Weber, Leon  and
      Litschko, Robert  and
      Artemova, Ekaterina  and
      Plank, Barbara",
    editor = "Henning, Sophie  and
      Stede, Manfred",
    booktitle = "Proceedings of The 18th Linguistic Annotation Workshop (LAW-XVIII), co-located with EACL",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2024.law-1.19/",
    pages = "197--215",
    abstract = "Instruction tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality problems in gold standard labels. So far, however, the application of AED methods has been limited to classification tasks. It is an open question how well AED methods generalize to language generation settings, which are becoming more widespread via LLMs. In this paper, we present a first and novel benchmark for AED on instruction tuning data: Donkii.It comprises three instruction-tuning datasets enriched with error annotations by experts and semi-automatic methods. We also provide a novel taxonomy of error types for instruction-tuning data.We find that all three datasets contain clear errors, which sometimes propagate directly into instruction-tuned LLMs. We propose four AED baselines for the generative setting and evaluate them extensively on the newly introduced dataset. Our results show that the choice of the right AED method and model size is indeed crucial and derive practical recommendations for how to use AED methods to clean instruction-tuning data.",
    preview = {donkii.png},
    slides = {donkii-slides.pdf},
    code = {https://github.com/mainlp/donkii},
    video = {https://drive.google.com/file/d/17_jJDrrrUwiy9T7ZwVT0olESHSSF6Vd8/view}
}


@inproceedings{litschko-etal-2023-establishing,
    title = "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
    author = {Litschko*, Robert  and
      MÃ¼ller-Eberstein*, Max  and
      van der Goot, Rob  and
      Weber-Genzel, Leon  and
      Plank, Barbara},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.emnlp-main.14/",
    pages = "193--203",
    abstract = "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model`s functional capacity, and provide recommendations for more multi-faceted evaluation protocols.",
    selected={true},
    preview={establishing-trustworthiness.png},
    video={https://drive.google.com/file/d/13AKbbFQH1OXDwa0ReppoTgmQzYjT2q0K/view},
    slides={emnlp23-trustworthiness-slides.pdf},
    poster={emnlp23-trustworthiness-poster.pdf}
}

@inproceedings{litschko-etal-2023-boosting,
    title = "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data",
    author = "Litschko, Robert  and
      Artemova, Ekaterina  and
      Plank, Barbara",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.findings-acl.193/",
    pages = "3096--3108",
    abstract = "Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distant languages (up to 2x absolute gain). We further show that our approach is robust towards the ratio of code-switched tokens and also extends to unseen languages. Our results demonstrate that training on code-switched data is a cheap and effective way of generalizing zero-shot rankers for cross-lingual and multilingual retrieval.",
   preview = {csclir_2.png},
   code = {https://github.com/mainlp/CodeSwitchCLIR},
   video={https://aclanthology.org/2023.findings-acl.193.mp4},
   slides={CSCLIR Presentation.pdf},
   poster={acl23-csclir-poster.pdf}
}

@inproceedings{de-la-pena-sarracen-etal-2023-vicinal,
    title = "Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection",
    author = "De la Pe{\~n}a Sarrac{\'e}n, Gretel  and
      Rosso, Paolo  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran  and
      Ponzetto, Simone",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.emnlp-main.248/",
    pages = "4069--4085",
    abstract = "Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.",
    video={https://aclanthology.org/2023.emnlp-main.248.mp4},
    preview={vicinal-risk-minimization.png},
    poster={poster_vrm.pdf}
}


@inproceedings{robert-litschko-etal-2023-general,
    title = "A General-Purpose Multilingual Document Encoder",
    author = "Robert Litschko, Onur Galo{\u{g}}lu  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL), co-located with EMNLP",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2023.mrl-1.4/",
    pages = "37--49",
    abstract = "Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate the effectiveness of HMDE in two arguably most common and prominent cross-lingual document-level tasks: (1) cross-lingual transfer for topical document classification and (2) cross-lingual document retrieval. HMDE is significantly more effective than (i) aggregations of segment-based representations and (ii) multilingual Longformer. Crucially, owing to its massively multilingual lower transformer, HMDE successfully generalizes to languages unseen in document-level pretraining.",
    preview = {general-purpose-document-encoder.png},
    code = {https://github.com/ogaloglu/pre-training-multilingual-document-encoders},
    poster={mrl_poster_hdme.pdf}
}


@inproceedings{litschko-etal-2022-parameter,
    title = "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
    author = "Litschko, Robert  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics (COLING)",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    pdf = "https://aclanthology.org/2022.coling-1.90/",
    pages = "1071--1082",
    abstract = "State-of-the-art neural (re)rankers are notoriously data-hungry which {--} given the lack of large-scale training data in languages other than English {--} makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore commonly transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all parameters of pretrained massively multilingual Transformers (MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy them in the target language(s). In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the (re)ranking adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. We carry out a large scale evaluation on the CLEF-2003 and HC4 benchmarks and additionally, as another contribution, extend the former with queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while being more modular and reducing training times. The gains are particularly pronounced for low-resource languages, where our approaches also substantially outperform the competitive machine translation-based rankers.",
    preview = {petclir2.png},
    code = {https://github.com/rlitschk/ModularCLIR},
    slides = {COLING 2022 presentation.pdf},
    video = {https://drive.google.com/file/d/1RCiuSAqaA2Lt4nDfxvB5vIO60oAWK-4g/view}
}

@inproceedings{hung-etal-2022-zusammenqa,
    title = "{Z}usammen{QA}: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System",
    author = "Hung, Chia-Chien  and
      Green, Tommaso  and
      Litschko, Robert  and
      Tsereteli, Tornike  and
      Takeshita, Sotaro  and
      Bombieri, Marco  and
      Glava{\v{s}}, Goran  and
      Ponzetto, Simone Paolo",
    editor = "Asai, Akari  and
      Choi, Eunsol  and
      Clark, Jonathan H.  and
      Hu, Junjie  and
      Lee, Chia-Hsuan  and
      Kasai, Jungo  and
      Longpre, Shayne  and
      Yamada, Ikuya  and
      Zhang, Rui",
    booktitle = "Proceedings of the Workshop on Multilingual Information Access (MIA), co-located with NAACL",
    month = jul,
    year = "2022",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2022.mia-1.8/",
    pages = "77--90",
    abstract = "This paper introduces our proposed system for the MIA Shared Task on Cross-lingual Openretrieval Question Answering (COQA). In this challenging scenario, given an input question the system has to gather evidence documents from a multilingual pool and generate from them an answer in the language of the question. We devised several approaches combining different model variants for three main components: Data Augmentation, Passage Retrieval, and Answer Generation. For passage retrieval, we evaluated the monolingual BM25 ranker against the ensemble of re-rankers based on multilingual pretrained language models (PLMs) and also variants of the shared task baseline, re-training it from scratch using a recently introduced contrastive loss that maintains a strong gradient signal throughout training by means of mixed negative samples. For answer generation, we focused on languageand domain-specialization by means of continued language model (LM) pretraining of existing multilingual encoders. Additionally, for both passage retrieval and answer generation, we augmented the training data provided by the task organizers with automatically generated question-answer pairs created from Wikipedia passages to mitigate the issue of data scarcity, particularly for the low-resource languages for which no training data were provided. Our results show that language- and domain-specialization as well as data augmentation help, especially for low-resource languages.",
    video={https://aclanthology.org/2022.mia-1.8.mp4},
    code={https://github.com/umanlp/zusammenqa},
    preview={zusammenqa.png},
    slides={NAACL2022_ZusammenQA.pdf}
}



@article{litschko2022cross,
  title={On cross-lingual retrieval with multilingual text encoders},
  author={Litschko, Robert and Vuli{\'c}, Ivan and Ponzetto, Simone Paolo and Glava{\v{s}}, Goran},
  journal={Information Retrieval Journal},
  volume={25},
  number={2},
  pages={149--183},
  year={2022},
  pdf={https://link.springer.com/article/10.1007/s10791-022-09406-x},
  publisher={Springer},
  abstract="In this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a number of diverse language pairs. We first treat these models as multilingual text encoders and benchmark their performance in unsupervised ad-hoc sentence- and document-level CLIR. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR -- a setup with no relevance judgments for IR-specific fine-tuning -- pretrained multilingual encoders on average fail to significantly outperform earlier models based on CLWEs. For sentence-level retrieval, we do obtain state-of-the-art performance: the peak scores, however, are met by multilingual encoders that have been further specialized, in a supervised fashion, for sentence understanding tasks, rather than using their vanilla 'off-the-shelf' variants. Following these results, we introduce localized relevance matching for document-level CLIR, where we independently score a query against document sections. In the second part, we evaluate multilingual encoders fine-tuned in a supervised fashion (i.e., we learn to rank) on English relevance data in a series of zero-shot language and domain transfer CLIR experiments. Our results show that supervised re-ranking rarely improves the performance of multilingual transformers as unsupervised base rankers. Finally, only with in-domain contrastive fine-tuning (i.e., same domain, only language transfer), we manage to improve the ranking quality. We uncover substantial empirical differences between cross-lingual retrieval results and results of (zero-shot) cross-lingual transfer for monolingual retrieval in target languages, which point to ``monolingual overfitting'' of retrieval models trained on monolingual data.",
  preview={irj-paper.png},
  code={https://github.com/rlitschk/EncoderCLIR}
}

@inproceedings{litschko2021evaluating,
  title={Evaluating multilingual text encoders for unsupervised cross-lingual retrieval},
  author={Litschko, Robert and Vuli{\'c}, Ivan and Ponzetto, Simone Paolo and Glava{\v{s}}, Goran},
  booktitle={Advances in Information Retrieval: 43rd European Conference on IR Research (ECIR)},
  pages={342--358},
  year={2021},
  organization={Springer},
  abstract="Pretrained multilingual text encoders based on neural Transformer architectures, such as multilingual BERT (mBERT) and XLM, have achieved strong performance on a myriad of language understanding tasks. Consequently, they have been adopted as a go-to paradigm for multilingual and cross-lingual representation learning and transfer, rendering cross-lingual word embeddings (CLWEs) effectively obsolete. However, questions remain to which extent this finding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual IR (CLIR) tasks. Therefore, in this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a large number of language pairs. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR -- a setup with no relevance judgments for IR-specific fine-tuning -- pretrained encoders fail to significantly outperform models based on CLWEs. For sentence-level CLIR, we demonstrate that state-of-the-art performance can be achieved. However, the peak performance is not met using the general-purpose multilingual text encoders `off-the-shelf', but rather relying on their variants that have been further specialized for sentence understanding tasks.",
  preview={evaluating-multilingual-textencoders.png},
  pdf={https://link.springer.com/chapter/10.1007/978-3-030-72113-8_23},
  code={https://github.com/rlitschk/EncoderCLIR},
  slides={ecir 2021 presentation.pdf},
  poster={ecir-2021-poster.pdf}
}

@inproceedings{vulic-etal-2020-probing,
    title = "Probing Pretrained Language Models for Lexical Semantics",
    author = "Vuli{\'c}, Ivan  and
      Ponti, Edoardo Maria  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran  and
      Korhonen, Anna",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/2020.emnlp-main.586/",
    pages = "7222--7240",
    abstract = "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
    preview = {lexical-semantics.png},
    video = {https://slideslive.com/38939098/probing-pretrained-language-models-for-lexical-semantics}
}

@inproceedings{litschko-etal-2020-towards,
    title = "Towards Instance-Level Parser Selection for Cross-Lingual Transfer of Dependency Parsers",
    author = "Litschko, Robert  and
      Vuli{\'c}, Ivan  and
      Agi{\'c}, {\v{Z}}eljko  and
      Glava{\v{s}}, Goran",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics (COLING)",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    pdf = "https://aclanthology.org/2020.coling-main.345/",
    pages = "3886--3898",
    abstract = "Current methods of cross-lingual parser transfer focus on predicting the best parser for a low-resource target language globally, that is, {\textquotedblleft}at treebank level{\textquotedblright}. In this work, we propose and argue for a novel cross-lingual transfer paradigm: instance-level parser selection (ILPS), and present a proof-of-concept study focused on instance-level selection in the framework of delexicalized parser transfer. Our work is motivated by an empirical observation that different source parsers are the best choice for different Universal POS-sequences (i.e., UPOS sentences) in the target language. We then propose to predict the best parser at the instance level. To this end, we train a supervised regression model, based on the Transformer architecture, to predict parser accuracies for individual POS-sequences. We compare ILPS against two strong single-best parser selection baselines (SBPS): (1) a model that compares POS n-gram distributions between the source and target languages (KL) and (2) a model that selects the source based on the similarity between manually created language vectors encoding syntactic properties of languages (L2V). The results from our extensive evaluation, coupling 42 source parsers and 20 diverse low-resource test languages, show that ILPS outperforms KL and L2V on 13/20 and 14/20 test languages, respectively. Further, we show that by predicting the best parser {\textquotedblleft}at treebank level{\textquotedblright} (SBPS), using the aggregation of predictions from our instance-level model, we outperform the same baselines on 17/20 and 16/20 test languages.",
    preview={sentselect.png},
    poster={coling2020-sentselect-poster.pdf}
}


@inproceedings{litschko2019evaluating,
  title={Evaluating resource-lean cross-lingual embedding models in unsupervised retrieval},
  author={Litschko, Robert and Glava{\v{s}}, Goran and Vulic, Ivan and Dietz, Laura},
  booktitle={Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval},
  pages={1109--1112},
  year={2019},
  abstract = "We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.",
  pdf={https://dl.acm.org/doi/10.1145/3331184.3331324},
  poster={poster_sigir19.pdf},
  abbr={SIGIR 19}
}


@inproceedings{glavas-etal-2019-properly,
    title = "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",
    author = "Glava{\v{s}}, Goran  and
      Litschko, Robert  and
      Ruder, Sebastian  and
      Vuli{\'c}, Ivan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pdf = "https://aclanthology.org/P19-1070/",
    pages = "710--721",
    abstract = "Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",
    preview = {how-to-eval.png},
    code = {https://github.com/codogogo/xling-eval},
    poster = {poster_acl19.pdf}
}


@inproceedings{litschko2018unsupervised,
  title={Unsupervised cross-lingual information retrieval using monolingual data only},
  author={Litschko, Robert and Glava{\v{s}}, Goran and Ponzetto, Simone Paolo and Vuli{\'c}, Ivan},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={1253--1256},
  year={2018},
  pdf={https://dl.acm.org/doi/10.1145/3209978.3210157},
  abstract="We propose a fully unsupervised framework for ad-hoc cross-lingual information retrieval (CLIR) which requires no bilingual data at all. The framework leverages shared cross-lingual word embedding spaces in which terms, queries, and documents can be represented, irrespective of their actual language. The shared embedding spaces are induced solely on the basis of monolingual corpora in two languages through an iterative process based on adversarial neural networks. Our experiments on the standard CLEF CLIR collections for three language pairs of varying degrees of language similarity (English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed fully unsupervised approach. Our CLIR models with unsupervised cross-lingual embeddings outperform baselines that utilize cross-lingual embeddings induced relying on word-level and document-level alignments. We then demonstrate that further improvements can be achieved by unsupervised ensemble CLIR models. We believe that the proposed framework is the first step towards development of effective CLIR models for language pairs and domains where parallel data are scarce or non-existent.",
  code={https://github.com/rlitschk/UnsupCLIR},
  poster={poster_sigir18.pdf},
  abbr={SIGIR 18}
}
